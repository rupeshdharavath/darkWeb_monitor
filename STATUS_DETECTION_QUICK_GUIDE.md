# ðŸŸ¢ðŸ”´ðŸŸ¡âš ï¸ URL Status Detection - Quick Reference\n\n## What Changed?\n\nYour darkweb monitor now detects and tracks **4 URL availability states**:\n\n### Status Types\n\n| Status | What It Means | Example |\n|--------|--------------|----------|\n| ðŸŸ¢ **ONLINE** | URL responded with HTTP 200 | Site accessible, content scraped |\n| ðŸ”´ **OFFLINE** | Connection refused | Tor network rejected connection |\n| ðŸŸ¡ **TIMEOUT** | Server didn't respond in 30s | Site down or DDosed |\n| âš ï¸ **ERROR** | Other network errors | Non-200 HTTP status, SSL errors |\n\n---\n\n## ðŸ“‚ Files Updated\n\n### 1. **app/scraper.py**\n- âœ… New `fetch_url()` function with granular exception handling\n- âœ… Returns `{status, content, response_time, status_code}`\n- âœ… Uses Tor proxy automatically\n- âœ… Tracks performance metrics\n\n### 2. **app/database.py**\n- âœ… New fields: `url_status`, `response_time`, `status_code`\n- âœ… New field: `status_history` (array of historic checks)\n- âœ… `insert_scraped_data()` now accepts status parameters\n\n### 3. **main.py**\n- âœ… Extracts status from scraper results\n- âœ… Passes status to database insertion\n- âœ… Displays status summary report at end:\n  ```\n  ðŸŸ¢ ONLINE  : 2\n  ðŸ”´ OFFLINE : 5\n  ðŸŸ¡ TIMEOUT : 1\n  âš ï¸  ERROR   : 0\n  ```\n\n---\n\n## ðŸ”„ Data Flow\n\n```python\n# Before (old system)\nscrape_url(session, url) â†’ returns content or None\n# Problem: Lost context about WHY it failed\n\n# After (new system)\nfetch_url(url) â†’ returns {status, content, response_time, status_code}\n# Solution: Full context about every outcome\n```\n\n---\n\n## ðŸ’¾ MongoDB Storage Example\n\n### Single Request (ONLINE)\n```json\n{\n  \"_id\": ObjectId(\"...\"),\n  \"url\": \"http://marketplace.onion\",\n  \"timestamp\": \"2026-02-21T14:30:45Z\",\n  \"url_status\": \"ONLINE\",\n  \"response_time\": 2.34,\n  \"status_code\": 200,\n  \"threat_score\": 75,\n  \"category\": \"Illegal Marketplace\",\n  \"risk_level\": \"HIGH\",\n  \"status_history\": [\n    {\n      \"timestamp\": \"2026-02-21T14:30:45Z\",\n      \"url_status\": \"ONLINE\",\n      \"response_time\": 2.34,\n      \"status_code\": 200\n    }\n  ]\n}\n```\n\n### Failed Request (OFFLINE)\n```json\n{\n  \"_id\": ObjectId(\"...\"),\n  \"url\": \"http://deadsite.onion\",\n  \"timestamp\": \"2026-02-21T14:32:10Z\",\n  \"url_status\": \"OFFLINE\",\n  \"response_time\": 30.01,\n  \"status_code\": null,\n  \"threat_score\": 0,\n  \"category\": \"Unknown\",\n  \"risk_level\": \"LOW\",\n  \"title\": \"[OFFLINE] Unable to fetch content\",\n  \"status_history\": [\n    {\n      \"timestamp\": \"2026-02-21T14:32:10Z\",\n      \"url_status\": \"OFFLINE\",\n      \"response_time\": 30.01,\n      \"status_code\": null\n    }\n  ]\n}\n```\n\n---\n\n## ðŸ” How to Query MongoDB\n\n### Find all ONLINE marketplaces\n```javascript\ndb.scraped_data.find({\n  \"url_status\": \"ONLINE\",\n  \"category\": \"Illegal Marketplace\"\n})\n```\n\n### Find sites that timeout frequently\n```javascript\ndb.scraped_data.find({\n  \"url_status\": \"TIMEOUT\"\n}).sort({response_time: -1})\n```\n\n### Get status history trend for a URL\n```javascript\ndb.scraped_data.findOne(\n  {\"url\": \"http://example.onion\"},\n  {\"url\": 1, \"status_history\": 1}\n)\n```\n\n### Find slowest sites (response_time > 10 seconds)\n```javascript\ndb.scraped_data.find({\n  \"response_time\": {\"$gt\": 10},\n  \"url_status\": \"ONLINE\"\n}).sort({response_time: -1})\n```\n\n### Find recently OFFLINE sites\n```javascript\ndb.scraped_data.find({\n  \"url_status\": \"OFFLINE\"\n}).sort({timestamp: -1}).limit(10)\n```\n\n---\n\n## ðŸ§ª Testing URL Status Detection\n\n### Manual Test\n```python\nfrom app.scraper import fetch_url\n\n# Test 1: ONLINE (works)\nresult = fetch_url(\"http://example.com\")\nprint(f\"Status: {result['status']}\")  # \"ONLINE\"\nprint(f\"Time: {result['response_time']:.2f}s\")\n\n# Test 2: OFFLINE (refused)\nresult = fetch_url(\"http://localhost:1234\")\nprint(f\"Status: {result['status']}\")  # \"OFFLINE\"\n\n# Test 3: TIMEOUT (slow server)\nresult = fetch_url(\"http://slowsite.onion\")\nprint(f\"Status: {result['status']}\")  # \"TIMEOUT\"\n```\n\n---\n\n## ðŸŽ¯ Why This Matters\n\n### For Threat Intelligence\n- **Availability patterns** = Law enforcement pressure\n- **Performance degradation** = DDos attack or overload\n- **Sudden going offline** = Possible takedown\n\n### For Your Demo\n- Shows **production-level error handling**\n- Demonstrates **graceful degradation**\n- Proves your system is **resilient** (doesn't crash on failures)\n- Provides **visual insights** (color-coded status)\n\n### For Enterprise Use\n- Track availability SLAs\n- Generate uptime reports\n- Alert on sudden status changes\n- Correlate with threat data\n\n---\n\n## âš¡ Performance Impact\n\n- **No performance hit**: Each URL fetched once per run\n- **Async-ready**: Can be parallelized in future versions\n- **Network timeouts**: 30 seconds per URL (adjustable)\n- **Total time**: ~30s per OFFLINE URL, ~2s per ONLINE URL\n\n---\n\n## ðŸš€ Next Steps\n\n1. **Run the system**: `python main.py`\n   - Watch the status summary appear at the end\n\n2. **Check MongoDB**: Query the `url_status` field\n   - See which sites are online/offline\n\n3. **Analyze trends**: Track status changes over time\n   - Notice when marketplaces go down\n\n4. **Correlate with threats**: High risk + ONLINE = Alert priority\n   - OFFLINE sites = Lower priority\n\n---\n\n## ðŸ“‹ Function Reference\n\n### fetch_url(url)\n```python\nfrom app.scraper import fetch_url\n\nresult = fetch_url(url)\n# Returns:\n# {\n#   \"status\": \"ONLINE|OFFLINE|TIMEOUT|ERROR\",\n#   \"content\": response text or None,\n#   \"response_time\": float seconds,\n#   \"status_code\": int or None\n# }\n```\n\n### insert_scraped_data(...)\n```python\ndb_manager.insert_scraped_data(\n    url,\n    parsed_data,\n    url_status=\"ONLINE\",        # NEW\n    response_time=2.34,         # NEW\n    status_code=200             # NEW\n)\n```\n\n---\n\n## ðŸŽ‰ Summary\n\nYour system now:\n- âœ… Classifies URL availability into 4 states\n- âœ… Tracks response times\n- âœ… Maintains status history\n- âœ… Never crashes on failures\n- âœ… Displays beautiful status report\n- âœ… Stores everything in MongoDB\n\n**This is professional-grade threat monitoring!**\n"